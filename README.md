# üéì Research Paper RAG System

> **An intelligent assistant for querying and understanding academic research papers using Retrieval-Augmented Generation (RAG)**

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14-black.svg)](https://nextjs.org/)
[![Qdrant](https://img.shields.io/badge/Qdrant-Vector_DB-red.svg)](https://qdrant.tech/)
[![Gemini](https://img.shields.io/badge/Google-Gemini-blue.svg)](https://ai.google.dev/)

## üìñ Overview

This is a production-ready RAG (Retrieval-Augmented Generation) system that enables researchers to efficiently query academic papers and get AI-powered answers with accurate citations. Instead of manually reading through multiple papers for hours, researchers can now get instant, contextual answers with page-specific citations.

### ‚ú® Key Features

- üìÑ **PDF Document Ingestion** - Upload research papers with automatic text extraction and intelligent chunking
- üîç **Semantic Search** - Vector-based similarity search using Qdrant for highly relevant content retrieval
- ü§ñ **AI-Powered Q&A** - Generate accurate answers using Google Gemini with proper citations
- üìä **Query Analytics** - Track query history, confidence scores, and response times
- ‚ö° **Smart Caching** - 700-1000x faster responses for repeated queries
- üé® **Modern Web UI** - Clean, responsive interface built with Next.js and Tailwind CSS

---

## üõ†Ô∏è Technology Stack

### Backend
- **Framework**: FastAPI (Python 3.11)
- **LLM**: Google Gemini API (gemini-2.5-flash)
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (384-dimensional)
- **Vector Database**: Qdrant (1.7+)
- **Metadata Storage**: SQLite
- **PDF Processing**: PyPDF2 + LangChain

### Frontend
- **Framework**: Next.js 14 (React 18)
- **Styling**: Tailwind CSS
- **Markdown Rendering**: react-markdown

### Infrastructure
- **Containerization**: Docker & Docker Compose
- **API Documentation**: Swagger/OpenAPI (FastAPI auto-generated)

---

## üìã Implemented Features

### ‚úÖ Core Functionality

#### 1. üìÑ Document Ingestion System
- Upload PDF research papers via REST API or Web UI
- Automatic metadata extraction (title, authors, year, page count)
- Section-aware chunking (Abstract, Introduction, Methods, Results, Discussion, Conclusion)
- Intelligent text splitting (500 chars with 50 char overlap)
- Vector embedding generation (384-dimensional)
- Batch storage in Qdrant with metadata payloads

**Performance**: Process 10-page papers in ~4 seconds

#### 2. üîç Intelligent Query System
- Natural language question answering
- Semantic search with configurable top-k results (1-20)
- Optional paper filtering (query specific papers only)
- Confidence scoring (0.0-1.0)
- Page-specific citations with relevance scores
- Response time tracking
- Query history persistence

**Response Format**:
```json
{
  "answer": "Detailed answer with markdown formatting...",
  "citations": [
    {
      "paper_title": "Attention is All You Need",
      "section": "Methods",
      "page": 3,
      "relevance_score": 0.89,
      "chunk_text": "Relevant excerpt..."
    }
  ],
  "sources_used": ["paper1.pdf", "paper2.pdf"],
  "confidence": 0.85,
  "response_time": 7.2,
  "cached": false
}
```

#### 3. üìö Paper Management
- `GET /api/papers` - List all uploaded papers
- `GET /api/papers/{id}` - Get paper details
- `DELETE /api/papers/{id}` - Remove paper and its vectors
- `GET /api/papers/{id}/stats` - View paper statistics
- Upload date tracking
- Processing status monitoring

#### 4. üìä Query History & Analytics
- `GET /api/queries/history` - Recent query history (paginated)
- `GET /api/analytics/popular` - Most queried topics
- Full query log with timestamps
- Confidence and response time metrics
- Source paper tracking

#### 5. ‚ö° Smart Caching System
- In-memory LRU cache with TTL (default: 1 hour)
- Query normalization (case-insensitive, whitespace-agnostic)
- 700-1000x speedup for repeated queries (< 0.01s vs 7-10s)
- Automatic cache invalidation on data changes
- `GET /api/cache/stats` - Cache performance metrics
- `POST /api/cache/clear` - Manual cache clearing

**Cache Hit Example**: Same query goes from 7.2s ‚Üí 0.001s ‚ö°

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Client Layer                             ‚îÇ
‚îÇ                  (Next.js + React + Tailwind)                   ‚îÇ
‚îÇ          Upload UI ‚îÇ Query Interface ‚îÇ History Dashboard         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ HTTP/REST
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   FastAPI Backend (Port 8000)                    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  PDF Processor     ‚îÇ           ‚îÇ    RAG Pipeline          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Extract text    ‚îÇ           ‚îÇ  - Query embedding       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Section detect  ‚îÇ           ‚îÇ  - Vector search         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Chunk (500ch)   ‚îÇ           ‚îÇ  - Context assembly      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Generate embed  ‚îÇ           ‚îÇ  - LLM generation        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ  - Citation extraction   ‚îÇ ‚îÇ
‚îÇ                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Cache Service     ‚îÇ           ‚îÇ    LLM Service           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Query normalize ‚îÇ           ‚îÇ  - Google Gemini API     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - LRU eviction    ‚îÇ           ‚îÇ  - Prompt engineering    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - TTL: 1 hour     ‚îÇ           ‚îÇ  - Response parsing      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                  ‚îÇ                  ‚îÇ
        ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Qdrant      ‚îÇ  ‚îÇ   SQLite      ‚îÇ  ‚îÇ  Google Gemini   ‚îÇ
‚îÇ   (Vector DB) ‚îÇ  ‚îÇ   (Metadata)  ‚îÇ  ‚îÇ  API (LLM)       ‚îÇ
‚îÇ               ‚îÇ  ‚îÇ               ‚îÇ  ‚îÇ                  ‚îÇ
‚îÇ - 384D vectors‚îÇ  ‚îÇ - Papers      ‚îÇ  ‚îÇ - gemini-2.5-    ‚îÇ
‚îÇ - Cosine sim  ‚îÇ  ‚îÇ - Queries     ‚îÇ  ‚îÇ   flash model    ‚îÇ
‚îÇ - Port 6333   ‚îÇ  ‚îÇ - Analytics   ‚îÇ  ‚îÇ - Rate: 15 RPM   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

**Upload Flow**:
```
PDF ‚Üí Extract Text ‚Üí Chunk ‚Üí Embed ‚Üí Store (Qdrant + SQLite)
```

**Query Flow**:
```
Question ‚Üí Check Cache ‚Üí [Miss] ‚Üí Embed ‚Üí Search Qdrant ‚Üí 
Generate Answer (Gemini) ‚Üí Extract Citations ‚Üí Cache ‚Üí Return
```

---

## üìä Project Structure

```
research-paper-rag-assessment/
‚îú‚îÄ‚îÄ src/                           # Backend (Python/FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Application entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ .env                       # Environment variables (API keys)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py              # REST API endpoints
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py            # SQLAlchemy ORM models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py             # Pydantic request/response schemas
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îú‚îÄ‚îÄ pdf_processor.py       # PDF extraction & chunking
‚îÇ       ‚îú‚îÄ‚îÄ embedding_service.py   # Text ‚Üí Vector embeddings
‚îÇ       ‚îú‚îÄ‚îÄ qdrant_client.py       # Vector database operations
‚îÇ       ‚îú‚îÄ‚îÄ llm_service.py         # Google Gemini integration
‚îÇ       ‚îú‚îÄ‚îÄ rag_pipeline.py        # RAG orchestration
‚îÇ       ‚îî‚îÄ‚îÄ cache_service.py       # Query caching logic
‚îÇ
‚îú‚îÄ‚îÄ frontend/                      # Frontend (Next.js)
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx               # Home page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload/page.tsx        # Upload interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query/page.tsx         # Query interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ papers/page.tsx        # Paper management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ history/page.tsx       # Query history
‚îÇ   ‚îú‚îÄ‚îÄ package.json               # Node dependencies
‚îÇ   ‚îî‚îÄ‚îÄ tailwind.config.ts         # Tailwind configuration
‚îÇ
‚îú‚îÄ‚îÄ sample_papers/                 # Test dataset (5 PDFs)
‚îú‚îÄ‚îÄ docker-compose.yml             # Multi-service orchestration
‚îú‚îÄ‚îÄ .gitignore                     # Git ignore rules (protects API keys)
‚îÇ
‚îú‚îÄ‚îÄ README.md                      # This file (main documentation)
‚îú‚îÄ‚îÄ APPROACH.md                    # Technical design decisions
‚îú‚îÄ‚îÄ CACHING.md                     # Caching system documentation
‚îî‚îÄ‚îÄ test_queries.json              # Sample test queries
```

---

## üöÄ Getting Started

### Prerequisites

Before running the application, ensure you have the following installed:

| Software | Version | Purpose | Download Link |
|----------|---------|---------|---------------|
| **Python** | 3.11+ | Backend runtime | [python.org](https://www.python.org/downloads/) |
| **Node.js** | 18+ | Frontend runtime | [nodejs.org](https://nodejs.org/) |
| **Docker Desktop** | Latest | Qdrant vector database | [docker.com](https://www.docker.com/products/docker-desktop/) |
| **Git** | Latest | Version control | [git-scm.com](https://git-scm.com/) |

**Required API Key**:
- Google Gemini API Key (Free tier available) - Get it from [Google AI Studio](https://makersuite.google.com/app/apikey)

---

## üì• Installation & Setup

### Step 1: Clone the Repository

```bash
git clone https://github.com/BadhonAhmad/research-paper-rag-assessment.git
cd research-paper-rag-assessment
```

### Step 2: Start Docker Desktop

**Windows/Mac**: Open Docker Desktop application and wait for it to start (green icon in system tray)

**Linux**:
```bash
sudo systemctl start docker
```

**Verify Docker is running**:
```bash
docker --version
docker ps
```

### Step 3: Start Qdrant Vector Database

Run Qdrant in a Docker container:

```bash
docker run -d -p 6333:6333 -p 6334:6334 ^
  -v qdrant_storage:/qdrant/storage ^
  --name qdrant ^
  qdrant/qdrant
```

**Verify Qdrant is running**:
- Open browser: http://localhost:6333/dashboard
- You should see the Qdrant web interface

### Step 4: Configure Backend Environment

Navigate to the backend directory and set up environment variables:

```bash
cd src
```

Create a `.env` file (or copy from `.env.example`):

```bash
# Windows PowerShell
Copy-Item .env.example .env

# Linux/Mac
cp .env.example .env
```

Edit `.env` and add your Gemini API key:

```env
# .env file
GEMINI_API_KEY=your_actual_api_key_here
GEMINI_MODEL=gemini-2.5-flash

QDRANT_HOST=localhost
QDRANT_PORT=6333

DATABASE_URL=sqlite:///./research_papers.db

CACHE_ENABLED=true
CACHE_TTL_SECONDS=3600
CACHE_MAX_SIZE=1000
```

### Step 5: Install Backend Dependencies

```bash
# Create virtual environment (recommended)
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate

# Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

**Dependencies installed**:
- FastAPI, Uvicorn (API framework)
- google-genai (Gemini API)
- sentence-transformers (embeddings)
- qdrant-client (vector database)
- PyPDF2, langchain (PDF processing)
- SQLAlchemy (ORM)

### Step 6: Start Backend Server

```bash
python main.py
```

**Expected output**:
```
üîß Initializing database...
‚úÖ Database initialized successfully
üîß Initializing services...
  Loading PDF processor...
  Loading embedding model...
  Connecting to Qdrant...
‚úÖ Qdrant collection exists: research_papers
  Initializing LLM service (Gemini)...
  Initializing RAG pipeline...
  Initializing query cache...

============================================================
üöÄ Starting Research Paper RAG System
============================================================
üì° API: http://0.0.0.0:8000
üìö Docs: http://0.0.0.0:8000/docs
üîç Qdrant: localhost:6333
ü§ñ LLM: Google Gemini (model: gemini-2.5-flash)
‚ö° Cache: ENABLED (TTL: 3600s, Max: 1000 queries)
============================================================

INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

**Verify backend is running**:
- Open browser: http://localhost:8000
- API docs: http://localhost:8000/docs

### Step 7: Install Frontend Dependencies

Open a **new terminal** (keep backend running):

```bash
cd frontend

# Install dependencies
npm install
```

**Dependencies installed**:
- Next.js 14, React 18
- Tailwind CSS
- react-markdown (for formatting AI responses)

### Step 8: Start Frontend Development Server

```bash
npm run dev
```

**Expected output**:
```
  ‚ñ≤ Next.js 14.0.4
  - Local:        http://localhost:3000
  - Environments: .env

 ‚úì Ready in 2.1s
```

**Access the application**:
- Open browser: http://localhost:3000

---

## üéØ Quick Start Guide

### 1Ô∏è‚É£ Upload Papers

1. Navigate to http://localhost:3000
2. Click **"Upload Papers"**
3. Select a PDF from `sample_papers/` directory
4. Wait for processing (~4 seconds per paper)
5. Repeat for all 5 sample papers

### 2Ô∏è‚É£ Ask Questions

1. Click **"Query Papers"**
2. Enter a question (e.g., "What methodology was used in the transformer paper?")
3. Adjust top-k results slider (default: 5)
4. Click **"Ask Question"**
5. View AI-generated answer with citations

### 3Ô∏è‚É£ View History

1. Click **"Query History"**
2. See all past queries with timestamps
3. Review confidence scores and response times

### 4Ô∏è‚É£ Manage Papers

1. Click **"View Papers"**
2. See all uploaded papers with metadata
3. Delete papers if needed (also removes vectors)

---

## ÔøΩ API Documentation

### Base URL
```
http://localhost:8000
```

### Interactive API Docs
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

### Core Endpoints

#### üìÑ Paper Management

**Upload Paper**
```http
POST /api/papers/upload
Content-Type: multipart/form-data

file: <PDF file>
```

**Response**:
```json
{
  "paper_id": 1,
  "title": "Attention is All You Need",
  "filename": "paper3_nlp_transformers.pdf",
  "total_pages": 15,
  "chunk_count": 42,
  "message": "Paper uploaded and processed successfully"
}
```

**List All Papers**
```http
GET /api/papers?skip=0&limit=100
```

**Get Paper Details**
```http
GET /api/papers/{paper_id}
```

**Delete Paper**
```http
DELETE /api/papers/{paper_id}
```

**Get Paper Statistics**
```http
GET /api/papers/{paper_id}/stats
```

#### üîç Query System

**Ask Question**
```http
POST /api/query
Content-Type: application/json

{
  "question": "What methodology was used in the transformer paper?",
  "top_k": 5,
  "paper_ids": null  // Optional: [1, 3] to filter specific papers
}
```

**Response**:
```json
{
  "answer": "The transformer paper introduces a novel architecture...",
  "citations": [
    {
      "paper_title": "Attention is All You Need",
      "section": "Methods",
      "page": 3,
      "relevance_score": 0.89,
      "chunk_text": "We propose a new simple network..."
    }
  ],
  "sources_used": ["paper3_nlp_transformers.pdf"],
  "confidence": 0.85,
  "response_time": 7.2,
  "cached": false
}
```

#### üìä Analytics

**Query History**
```http
GET /api/queries/history?skip=0&limit=50
```

**Popular Topics**
```http
GET /api/analytics/popular?limit=10
```

**Cache Statistics**
```http
GET /api/cache/stats
```

**Clear Cache**
```http
POST /api/cache/clear
```

#### üè• Health Check

```http
GET /health
```

**Response**:
```json
{
  "status": "healthy",
  "services": {
    "pdf_processor": true,
    "embedding_service": true,
    "qdrant": true,
    "rag_pipeline": true,
    "query_cache": true
  }
}
```

### Example cURL Commands

```bash
# Upload a paper
curl -X POST "http://localhost:8000/api/papers/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@sample_papers/paper1_machine_learning.pdf"

# Ask a question
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is machine learning?",
    "top_k": 5
  }'

# Get query history
curl -X GET "http://localhost:8000/api/queries/history?limit=10"

# Check cache stats
curl -X GET "http://localhost:8000/api/cache/stats"
```

---

## üß™ Testing

### Test with Sample Papers

5 sample research papers are provided in `sample_papers/`:

1. `paper1_machine_learning.pdf` - Classic ML algorithms
2. `paper2_neural_networks.pdf` - Deep learning architectures
3. `paper3_nlp_transformers.pdf` - Transformer models
4. `paper4_computer_vision.pdf` - CNN and vision models
5. `paper5_reinforcement_learning.pdf` - RL algorithms

### Test Queries

20 test queries are provided in `test_queries.json`:

```json
[
  {
    "query": "What is machine learning?",
    "difficulty": "easy",
    "expected_papers": ["paper1"]
  },
  {
    "query": "Compare CNNs and Transformers for image classification",
    "difficulty": "hard",
    "expected_papers": ["paper3", "paper4"]
  }
]
```

### Running Tests

```bash
# Upload all sample papers
python scripts/upload_test_papers.py

# Run test queries
python scripts/test_queries.py

# Check results
python scripts/evaluate_results.py
```

---

## üìà Performance Metrics

### Upload Performance
- **10-page paper**: ~4 seconds
- **PDF extraction**: 1.2s (30%)
- **Embedding**: 2.3s (57%)
- **Vector storage**: 0.5s (13%)

### Query Performance
- **Cold query** (no cache): 7-10 seconds
  - Embedding: 0.2s (2%)
  - Vector search: 0.3s (4%)
  - LLM generation: 7.2s (86%)
  - Database save: 0.1s (1%)
- **Cached query**: < 0.01 seconds ‚ö° (700-1000x faster!)

### Cache Statistics
- **Hit rate**: 40-60% (typical usage)
- **TTL**: 1 hour (configurable)
- **Max size**: 1000 queries
- **Memory usage**: ~2 MB

---

## üõ°Ô∏è Security

### API Key Protection

**‚ùå NEVER commit these files:**
- `.env`
- `.env.local`
- `.env.docker`
- `src/.env`

These are already in `.gitignore`.

**‚úÖ Safe to commit:**
- `.env.example` (with placeholder values)

### If You Accidentally Exposed Your API Key

1. **Revoke the key immediately**:
   - Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Delete the exposed key
   - Generate a new one

2. **Update your local `.env`**:
   ```env
   GEMINI_API_KEY=your_new_key_here
   ```

3. **Remove from git history** (if already pushed):
   ```bash
   # Use BFG Repo-Cleaner or git filter-branch
   # Contact repository maintainer if needed
   ```

---

## üîç Troubleshooting

### Backend Issues

**Problem**: `ModuleNotFoundError: No module named 'fastapi'`
```bash
# Solution: Install dependencies
pip install -r requirements.txt
```

**Problem**: `ConnectionError: Could not connect to Qdrant`
```bash
# Solution: Ensure Qdrant is running
docker ps
docker run -d -p 6333:6333 qdrant/qdrant
```

**Problem**: `google.api_core.exceptions.InvalidArgument: API key not valid`
```bash
# Solution: Check your .env file
cat src/.env  # Linux/Mac
type src\.env  # Windows

# Verify GEMINI_API_KEY is set correctly
```

### Frontend Issues

**Problem**: `Module not found: Can't resolve 'react-markdown'`
```bash
# Solution: Install dependencies
cd frontend
npm install
```

**Problem**: `Error: listen EADDRINUSE: address already in use :::3000`
```bash
# Solution: Kill process using port 3000
# Windows:
netstat -ano | findstr :3000
taskkill /PID <PID> /F

# Linux/Mac:
lsof -ti:3000 | xargs kill -9
```

---

## üìö Additional Documentation

- **[APPROACH.md](APPROACH.md)** - Detailed technical design decisions
  - Chunking strategy (why 500 chars?)
  - Embedding model choice (why all-MiniLM-L6-v2?)
  - Prompt engineering techniques
  - Database schema rationale
  - Trade-offs and limitations

- **[CACHING.md](CACHING.md)** - Query caching system documentation
  - How caching works
  - Performance benefits
  - Configuration options
  - Cache management API

- **[SUBMISSION_GUIDE.md](SUBMISSION_GUIDE.md)** - How to submit your work

- **[PULL_REQUEST_TEMPLATE.md](PULL_REQUEST_TEMPLATE.md)** - PR guidelines

---

## ü§ù Contributing

This project was developed as part of a technical assessment. For improvements or bug fixes:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit your changes (`git commit -m 'Add some improvement'`)
4. Push to the branch (`git push origin feature/improvement`)
5. Open a Pull Request

---

## üìÑ License

This project is part of a technical assessment. All rights reserved.

---

## üë§ Author

**Nobel Badhon Ahmad**
- GitHub: [@BadhonAhmad](https://github.com/BadhonAhmad)
- Branch: `submission/Nobel-Badhon`

---

## ÔøΩ Acknowledgments

- Google Gemini API for LLM capabilities
- Qdrant for efficient vector search
- LangChain for RAG utilities
- sentence-transformers for embedding models
- FastAPI for the excellent web framework
- Next.js team for the React framework

---

## üìû Support

For questions or issues:
- üìß Email: ahmadbadhon28@gmail.com
- üêõ GitHub Issues: [Create an issue](https://github.com/BadhonAhmad/research-paper-rag-assessment/issues)

---

**Built with ‚ù§Ô∏è for AI-powered research assistance**